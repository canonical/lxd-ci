#!/bin/bash
set -eux

# testflinger_queue: rockman

# Install required components from "restricted" pocket
if ! grep -v '^#' /etc/apt/sources.list | grep -qwFm1 restricted; then
    ARCH="$(dpkg --print-architecture)"
    DISTRO="$(lsb_release -sc)"
    if [ "$ARCH" != "amd64" ]; then
        cat << EOF > /etc/apt/sources.list.d/restricted.list
deb [arch=${ARCH}] http://ports.ubuntu.com/ubuntu-ports ${DISTRO} restricted
deb [arch=${ARCH}] http://ports.ubuntu.com/ubuntu-ports ${DISTRO}-updates restricted
EOF
    else
        cat << EOF > /etc/apt/sources.list.d/restricted.list
deb [arch=${ARCH}] http://archive.ubuntu.com/ubuntu/ ${DISTRO} restricted
deb [arch=${ARCH}] http://archive.ubuntu.com/ubuntu/ ${DISTRO}-updates restricted
EOF
    fi
fi

if mokutil --sb-state | grep -Fx "SecureBoot enabled"; then
  echo "SecureBoot needs to be disabled to avoid a prompt to register custom MOK (Machine-Owner Key) during DKMS" >&2
  exit 1
fi

# Install dependencies
install_deps jq ubuntu-drivers-common
RECOMMENDED_DRIVER="$(ubuntu-drivers devices 2>/dev/null | awk '/nvidia-driver-.*recommended$/ {print $3}')"
INSTALL_RECOMMENDS=yes install_deps "${RECOMMENDED_DRIVER}"

# Install LXD
install_lxd

IMAGE="${TEST_IMG:-ubuntu-daily:24.04}"

# Check that NVIDIA is installed
nvidia-smi

# Configure LXD
lxc storage create default dir
lxc profile device add default root disk path=/ pool=default
lxc network create lxdbr0
lxc profile device add default eth0 nic network=lxdbr0 name=eth0

# Consult available resources
total_gpu="$(lxc query /1.0/resources | jq -r '.gpu.total')"
total_nvidia_gpu="$(lxc query /1.0/resources | jq -r '.gpu.cards | .[] | select(.driver == "nvidia") | .pci_address' | wc -l)"
first_card_pci_slot="$(lxc query /1.0/resources | jq -r '.gpu.cards | .[] | select(.driver == "nvidia") | .pci_address' | head -n1)"
first_card_product_id="$(lxc query /1.0/resources | jq -r ".gpu.cards | .[] | select(.pci_address == \"${first_card_pci_slot}\") | .product_id")"
total_nvidia_gpu_with_product_id="$(lxc query /1.0/resources | jq -r ".gpu.cards | .[] | select(.product_id == \"${first_card_product_id}\") | .product_id" | wc -l)"

# Check if available resources are sufficient
[ "${total_gpu}" -gt 1 ]
[ "${total_nvidia_gpu}" -ge 1 ]
[ "${total_nvidia_gpu_with_product_id}" -ge 1 ]

# Launch a test container
echo "==> Launching a test container"
lxc launch "${IMAGE}" c1
waitInstanceReady c1

# Confirm no GPU
echo "==> Testing with no GPU"
! lxc exec c1 -- ls -lh /dev/dri/ || false

# Validate with one GPU
echo "==> Testing with one GPU"
lxc config device add c1 gpu0 gpu id=0
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ]

# Validate with all remove
echo "==> Testing with no GPU"
lxc config device remove c1 gpu0
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "0" ]

# Validate with all GPUs
echo "==> Testing with all NVIDIA GPUs"
lxc config device add c1 gpus gpu
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_gpu}" ]

# Test nvidia runtime
echo "==> Testing nvidia runtime"
! lxc exec c1 -- nvidia-smi || false
lxc stop -f c1
lxc config set c1 nvidia.runtime true
lxc start c1
waitInstanceReady c1
lxc exec c1 -- nvidia-smi

# Test with PCI addresses
echo "==> Testing PCI address selection"
lxc config device remove c1 gpus
lxc config device add c1 gpu1 gpu pci="${first_card_pci_slot}"
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ]
lxc exec c1 -- nvidia-smi

# Test with vendor
echo "==> Testing PCI vendor selection"
lxc config device remove c1 gpu1
lxc config device add c1 gpus gpu vendorid=10de
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_nvidia_gpu}" ]
lxc exec c1 -- nvidia-smi

# Test with vendor and product
echo "==> Testing PCI vendor and product selection"
lxc config device remove c1 gpus
lxc config device add c1 gpus gpu vendorid=10de productid="${first_card_product_id}"
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_nvidia_gpu_with_product_id}" ]
lxc exec c1 -- nvidia-smi

# Test with fully-qualified CDI name
echo "==> Testing adding a GPU with a fully-qualified CDI name"
lxc config unset c1 nvidia.runtime
lxc stop --force c1
lxc config device remove c1 gpus
lxc config device add c1 gpu0 gpu id="nvidia.com/gpu=0"
lxc start c1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ]
lxc exec c1 -- nvidia-smi

echo "==> Cleaning up"
lxc delete -f c1
lxc profile device remove default root
lxc profile device remove default eth0
lxc storage delete default
lxc network delete lxdbr0

# shellcheck disable=SC2034
FAIL=0
