#!/bin/bash
set -eux

# testflinger_queue: rockman

# Install required components from "restricted" pocket
if ! grep -v '^#' /etc/apt/sources.list | grep -qwFm1 restricted; then
    ARCH="$(dpkg --print-architecture)"
    DISTRO="$(lsb_release -sc)"
    if [ "$ARCH" != "amd64" ]; then
        cat << EOF > /etc/apt/sources.list.d/restricted.list
deb [arch=${ARCH}] http://ports.ubuntu.com/ubuntu-ports ${DISTRO} restricted
deb [arch=${ARCH}] http://ports.ubuntu.com/ubuntu-ports ${DISTRO}-updates restricted
EOF
    else
        cat << EOF > /etc/apt/sources.list.d/restricted.list
deb [arch=${ARCH}] http://archive.ubuntu.com/ubuntu/ ${DISTRO} restricted
deb [arch=${ARCH}] http://archive.ubuntu.com/ubuntu/ ${DISTRO}-updates restricted
EOF
    fi
fi

if mokutil --sb-state | grep -Fx "SecureBoot enabled"; then
  echo "SecureBoot needs to be disabled to avoid a prompt to register custom MOK (Machine-Owner Key) during DKMS" >&2
  exit 1
fi

# Install dependencies
install_deps jq ubuntu-drivers-common
RECOMMENDED_DRIVER="$(ubuntu-drivers devices 2>/dev/null | awk '/nvidia-driver-.*recommended$/ {print $3}')"
INSTALL_RECOMMENDS=yes install_deps "${RECOMMENDED_DRIVER}"

# Install LXD
install_lxd

IMAGE="${TEST_IMG:-ubuntu-daily:24.04}"

# Check that NVIDIA is installed
nvidia-smi

# Configure LXD
lxc storage create default zfs
lxc profile device add default root disk path=/ pool=default
lxc network create lxdbr0
lxc profile device add default eth0 nic network=lxdbr0 name=eth0

# Consult available resources
total_gpu="$(lxc query /1.0/resources | jq -r '.gpu.total')"
total_nvidia_gpu="$(lxc query /1.0/resources | jq -r '.gpu.cards | .[] | select(.driver == "nvidia") | .pci_address' | wc -l)"
first_card_pci_slot="$(lxc query /1.0/resources | jq -r '.gpu.cards | .[] | select(.driver == "nvidia") | .pci_address' | head -n1)"
first_card_product_id="$(lxc query /1.0/resources | jq -r ".gpu.cards | .[] | select(.pci_address == \"${first_card_pci_slot}\") | .product_id")"
total_nvidia_gpu_with_product_id="$(lxc query /1.0/resources | jq -r ".gpu.cards | .[] | select(.product_id == \"${first_card_product_id}\") | .product_id" | wc -l)"

# Check if available resources are sufficient
[ "${total_gpu}" -gt 1 ]
[ "${total_nvidia_gpu}" -ge 1 ]
[ "${total_nvidia_gpu_with_product_id}" -ge 1 ]

# Launch a test container
echo "==> Launching a test container"
lxc launch "${IMAGE}" c1
waitInstanceReady c1

# Confirm no GPU
echo "==> Testing with no GPU"
! lxc exec c1 -- ls -lh /dev/dri/ || false

# Validate with one GPU
echo "==> Testing with one GPU"
lxc config device add c1 gpu0 gpu id=0
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ] || false

# Validate with all remove
echo "==> Testing with no GPU"
lxc config device remove c1 gpu0
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "0" ] || false

# Validate with all GPUs
echo "==> Testing with all NVIDIA GPUs"
lxc config device add c1 gpus gpu
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_gpu}" ] || false

# Test nvidia runtime
echo "==> Testing nvidia runtime"
! lxc exec c1 -- nvidia-smi || false
lxc stop -f c1
lxc config set c1 nvidia.runtime true
lxc start c1
waitInstanceReady c1
lxc exec c1 -- nvidia-smi

# Test with PCI addresses
echo "==> Testing PCI address selection"
lxc config device remove c1 gpus
lxc config device add c1 gpu1 gpu pci="${first_card_pci_slot}"
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ] || false
lxc exec c1 -- nvidia-smi

# Test with vendor
echo "==> Testing PCI vendor selection"
lxc config device remove c1 gpu1
lxc config device add c1 gpus gpu vendorid=10de
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_nvidia_gpu}" ] || false
lxc exec c1 -- nvidia-smi

# Test with vendor and product
echo "==> Testing PCI vendor and product selection"
lxc config device remove c1 gpus
lxc config device add c1 gpus gpu vendorid=10de productid="${first_card_product_id}"
sleep 1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "${total_nvidia_gpu_with_product_id}" ] || false
lxc exec c1 -- nvidia-smi

# Test with fully-qualified CDI name
echo "==> Testing adding a GPU with a fully-qualified CDI name"
lxc config unset c1 nvidia.runtime
lxc stop --force c1
lxc config device remove c1 gpus
lxc config device add c1 gpu0 gpu id="nvidia.com/gpu=0"
lxc start c1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "1" ] || false
lxc exec c1 -- nvidia-smi

echo "==> Testing adding multiple GPUs with fully-qualified CDI names"
lxc stop --force c1
lxc config device add c1 gpu1 gpu id="nvidia.com/gpu=1"
lxc start c1
[ "$(lxc exec c1 -- ls /dev/dri/ | grep -c '^card[0-9]')" = "2" ] || false
lxc exec c1 -- nvidia-smi
lxc delete -f c1

# Check that CDI device files are cleanly removed even if the host machine is abruptly rebooted
echo "==> Testing that CDI device files are cleanly removed after abrupt reboot"
lxc init "${IMAGE}" c1
if hasNeededAPIExtension devlxd_images; then
  lxc config set c1 -c security.devlxd.images=true
fi

lxc config device add c1 gpu0 gpu pci="${first_card_pci_slot}"
lxc start c1
echo "==> Waiting for the container to be ready"
waitInstanceReady c1

echo "==> Installing NVIDIA drivers inside the container"
lxc exec c1 -- apt-get update
lxc exec c1 --env DEBIAN_FRONTEND=noninteractive -- apt-get install -y ubuntu-drivers-common
lxc exec c1 --env DEBIAN_FRONTEND=noninteractive -- ubuntu-drivers autoinstall

echo "==> Rebooting the container to load NVIDIA drivers"
lxc restart c1

waitInstanceReady c1

echo "==> Verifying NVIDIA driver installation in the container"
lxc exec c1 -- nvidia-smi

echo "==> Installing LXD inside the container"
lxc exec c1 -- snap install lxd --channel="${LXD_SNAP_CHANNEL}"

echo "==> Initializing LXD inside the container"
lxc exec c1 -- lxd init --auto

echo "==> Launching a nested container inside the container"
lxc exec c1 -- lxc init "${IMAGE}" c11

echo "==> Adding GPU to the nested container inside the container using CDI"
lxc exec c1 -- lxc config device add c11 gpu0 gpu id="nvidia.com/gpu=0"
lxc exec c1 -- lxc start c11
# Wait for the container to be ready
sleep 20

echo "==> Verifying GPU access inside the nested container"
lxc exec c1 -- lxc exec c11 -- nvidia-smi

echo "==> Simulating abrupt reboot by force-stopping the container"
lxc stop c1 -f

echo "==> Starting the container again"
lxc start c1

waitInstanceReady c1

echo "==> Starting the nested container inside the container after reboot"
lxc exec c1 -- lxc start c11

echo "==> Verifying GPU access inside the nested container after container reboot"
lxc exec c1 -- lxc exec c11 -- nvidia-smi

echo "==> Cleaning up"
lxc delete -f c1
lxc profile device remove default root
lxc profile device remove default eth0
lxc storage delete default
lxc network delete lxdbr0

# shellcheck disable=SC2034
FAIL=0
