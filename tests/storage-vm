#!/bin/sh
set -eux

# testflinger_queue: hardhat

# Install dependencies
install_deps jq

# Install LXD
install_lxd

# XXX: skip ceph for now
poolDriverList="${1:-dir btrfs lvm lvm-thin zfs}"

# Configure LXD
lxc network create lxdbr0
lxc profile device add default eth0 nic network=lxdbr0

poolName="vmpool$$"

GiB=1073741823

for poolDriver in $poolDriverList; do
        echo "::group::${poolDriver}"
        echo "==> Create storage pool using driver ${poolDriver}"
        if [ "${poolDriver}" = "dir" ] || [ "${poolDriver}" = "ceph" ]; then
                lxc storage create "${poolName}" "${poolDriver}"
        elif [ "${poolDriver}" = "lvm" ]; then
                lxc storage create "${poolName}" "${poolDriver}" size=60GiB lvm.use_thinpool=false
        elif [ "${poolDriver}" = "lvm-thin" ]; then
                lxc storage create "${poolName}" lvm size=20GiB
        elif [ "${poolDriver}" = "powerflex" ]; then
                createPowerFlexPool "${poolName}"
        else
                lxc storage create "${poolName}" "${poolDriver}" size=20GiB
        fi

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Create VM and boot with small root (3GiB)"
                lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}" -d root,size=3GiB
        else
                echo "==> Create VM and boot with small root (8GiB)"
                lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}" -d root,size=8GiB
        fi
        lxc start v1
        waitInstanceReady v1
        lxc info v1

        echo "==> Check /dev/disk/by-id"
        lxc exec v1 -- test -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_lxd_root
        lxc exec v1 -- test -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_lxd_root-part1
        if lxc exec v1 -- mount | grep -qwF /boot/efi; then
            lxc exec v1 -- test -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_lxd_root-part15
        fi

        echo "==> Check config drive is readonly"
        # Check 9p config drive share is exported readonly.
        lxc exec v1 -- mount -t 9p config /srv
        ! lxc exec v1 -- touch /srv/lxd-test || false
        lxc exec v1 -- umount /srv

        # Check virtiofs config drive share is exported readonly.
        lxc exec v1 -- mount -t virtiofs config /srv
        ! lxc exec v1 -- touch /srv/lxd-test || false
        lxc exec v1 -- umount /srv

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking VM root disk size is 3GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "3" ]
        else
                echo "==> Checking VM root disk size is 8GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "8" ]
        fi

        echo "foo" | lxc exec v1 -- tee /root/foo.txt
        lxc exec v1 -- sync
        # Add file to the VM's filesystem volume.
        nsenter --mount=/run/snapd/ns/lxd.mnt touch "/var/snap/lxd/common/lxd/storage-pools/${poolName}/virtual-machines/v1/foo"
        lxc snapshot v1
        # Remove the file from filesytem volume after snapshot.
        nsenter --mount=/run/snapd/ns/lxd.mnt rm "/var/snap/lxd/common/lxd/storage-pools/${poolName}/virtual-machines/v1/foo"

        echo "==> Checking restore VM snapshot"
        lxc restore v1 snap0
        waitInstanceReady v1
        lxc exec v1 -- cat /root/foo.txt | grep -Fx "foo"

        echo "==> Checking VM filesystem volume was restored too"
        nsenter --mount=/run/snapd/ns/lxd.mnt [ -f "/var/snap/lxd/common/lxd/storage-pools/${poolName}/virtual-machines/v1/foo" ]

        echo "==> Checking VM can be copied with snapshots"
        lxc copy v1 v2
        [ "$(lxc query /1.0/instances/v2?recursion=1 | jq '.snapshots | length')" -eq "1" ]
        lxc start v2
        waitInstanceReady v2
        lxc delete -f v2

        echo "==> Checking running copied VM snapshot"
        lxc copy v1/snap0 v2
        lxc start v2
        waitInstanceReady v2
        lxc exec v2 -- cat /root/foo.txt | grep -Fx "foo"

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking VM snapshot copy root disk size is 3GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "3" ]
        else
                echo "==> Checking VM snapshot copy root disk size is 8GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "8" ]
        fi
        lxc delete -f v2
        lxc delete v1/snap0

        echo "==> Check QEMU crash behavior and recovery"
        lxc exec v1 -- fsfreeze --freeze /
        uuid=$(lxc config get v1 volatile.uuid)
        pgrep -af "${uuid}"
        nsenter --mount=/run/snapd/ns/lxd.mnt -- rm /var/snap/lxd/common/lxd/logs/v1/qemu.monitor
        systemctl reload snap.lxd.daemon
        sleep 5
        lxc ls v1 | grep ERROR
        ! lxc stop v1 || false
        ! lxc start v1 || false
        pgrep -af "${uuid}"
        lxc stop v1 -f
        ! pgrep -af "${uuid}" || false
        lxc start v1
        waitInstanceReady v1

        echo "==> Testing VM non-optimized export/import (while running to check config.mount is excluded)"
        lxc exec v1 -- fsfreeze --freeze /
        lxc export v1 "/tmp/lxd-test-${poolName}.tar.gz"
        lxc delete -f v1
        lxc import "/tmp/lxd-test-${poolName}.tar.gz"
        rm "/tmp/lxd-test-${poolName}.tar.gz"
        lxc start v1
        waitInstanceReady v1

        echo "==> Testing VM optimized export/import (while running to check config.mount is excluded)"
        lxc exec v1 -- fsfreeze --freeze /
        lxc export v1 "/tmp/lxd-test-${poolName}-optimized.tar.gz" --optimized-storage
        lxc delete -f v1
        lxc import "/tmp/lxd-test-${poolName}-optimized.tar.gz"
        rm "/tmp/lxd-test-${poolName}-optimized.tar.gz"
        lxc start v1
        waitInstanceReady v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Increasing VM root disk size for next boot (4GiB)"
                lxc config device set v1 root size=4GiB
        else
                echo "==> Increasing VM root disk size for next boot (16GiB)"
                lxc config device set v1 root size=16GiB
        fi
        lxc config get v1 volatile.root.apply_quota | grep true
        lxc stop -f v1
        lxc start v1
        waitInstanceReady v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking VM root disk size is 4GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "4" ]

                echo "==> Check VM shrink is blocked"
                ! lxc config device set v1 root size=3GiB || false
        else
                echo "==> Checking VM root disk size is 16GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "16" ]

                echo "==> Check VM shrink is blocked"
                ! lxc config device set v1 root size=8GiB || false
        fi

        echo "==> Checking additional disk device support"
        lxc stop -f v1

        # Create directory with a file for directory disk tests.
        mkdir "/tmp/lxd-test-${poolName}"
        touch "/tmp/lxd-test-${poolName}/lxd-test"

        # Create empty block file for block disk tests.
        truncate -s 5m "/tmp/lxd-test-${poolName}/lxd-test-block"

        # Add disks
        lxc config device add v1 dir1rw disk source="/tmp/lxd-test-${poolName}" path="/srv/rw"
        lxc config device add v1 dir1ro disk source="/tmp/lxd-test-${poolName}" path="/srv/ro" readonly=true
        lxc config device add v1 block1ro disk source="/tmp/lxd-test-${poolName}/lxd-test-block" readonly=true
        lxc config device add v1 block1rw disk source="/tmp/lxd-test-${poolName}/lxd-test-block"
        lxc start v1
        waitInstanceReady v1

        echo "==> Testing VM lxd-agent drive mounts"
        # Check there is only 1 mount for each directory disk and that it is mounted with the appropriate options.
        lxc exec v1 -- mount | grep -cF '/srv/rw type' | grep -xF 1
        lxc exec v1 -- mount | grep -cF '/srv/ro type' | grep -xF 1

        # RW disks should use virtio-fs when used with the snap.
        lxc exec v1 -- mount | grep 'lxd_dir1rw on /srv/rw type virtiofs (rw,relatime)'

        # RO disks should use virtio-fs when used with the snap but be mounted readonly.
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro type virtiofs (ro,relatime)'

        # Check UID/GID are correct.
        lxc exec v1 -- stat -c '%u:%g' /srv/rw | grep -xF '0:0'
        lxc exec v1 -- stat -c '%u:%g' /srv/ro | grep -xF '0:0'

        # Remount the readonly disk as rw inside VM and check that the disk is still readonly at the LXD layer.
        lxc exec v1 -- mount -oremount,rw /srv/ro
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro type virtiofs (rw,relatime)'
        ! lxc exec v1 -- touch /srv/ro/lxd-test-ro || false
        ! lxc exec v1 -- mkdir /srv/ro/lxd-test-ro || false
        ! lxc exec v1 -- rm /srv/ro/lxd-test.txt || false
        ! lxc exec v1 -- chmod 777 /srv/ro || false

        ## Mount the readonly disk as rw inside VM using 9p and check the disk is still readonly at the LXD layer.
        lxc exec v1 -- mkdir /srv/ro9p
        lxc exec v1 -- mount -t 9p lxd_dir1ro /srv/ro9p
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro9p type 9p (rw,relatime,sync,dirsync,access=client,trans=virtio)'
        ! lxc exec v1 -- touch /srv/ro9p/lxd-test-ro || false
        ! lxc exec v1 -- mkdir /srv/ro9p/lxd-test-ro || false
        ! lxc exec v1 -- rm /srv/ro9p/lxd-test.txt || false
        ! lxc exec v1 -- chmod 777 /srv/ro9p || false

        # Check writable disk is writable.
        lxc exec v1 -- touch /srv/rw/lxd-test-rw
        stat -c '%u:%g' "/tmp/lxd-test-${poolName}/lxd-test-rw" | grep -xF "0:0"
        lxc exec v1 -- rm /srv/rw/lxd-test-rw
        lxc exec v1 -- rm /srv/rw/lxd-test

        # Check block disks are available.
        lxc exec v1 -- stat -c "%F" /dev/sdb | grep -xF "block special file"
        lxc exec v1 -- stat -c "%F" /dev/sdc | grep -xF "block special file"

        # Check the rw driver accepts writes and the ro does not.
        ! lxc exec v1 -- dd if=/dev/urandom of=/dev/sdb bs=512 count=2 || false
        lxc exec v1 -- dd if=/dev/urandom of=/dev/sdc bs=512 count=2

        # Remove temporary directory (should now be empty aside from block file).
        echo "==> Stopping VM"
        lxc stop -f v1
        rm "/tmp/lxd-test-${poolName}/lxd-test-block"
        rmdir "/tmp/lxd-test-${poolName}"

        echo "==> Deleting VM"
        lxc delete -f v1

        # Create directory with a file for directory disk tests.
        mkdir "/tmp/lxd-test-${poolName}"

        # Create empty block file for block disk tests.
        truncate -s 5m "/tmp/lxd-test-${poolName}/lxd-test-block"

        echo "==> Checking disk device hotplug support"
        lxc launch "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}"
        waitInstanceReady v1

        # Hotplug disks
        if [ "${poolDriver}" != "powerflex" ]; then
                lxc storage volume create "${poolName}" vol1 --type=block size=10MB
        else
                lxc storage volume create "${poolName}" vol1 --type=block size=8GiB
        fi
        lxc storage volume attach "${poolName}" vol1 v1
        sleep 3
        lxc exec v1 -- stat -c "%F" /dev/sdb | grep -xF "block special file"
        lxc storage volume detach "${poolName}" vol1 v1
        sleep 3
        ! lxc exec v1 -- stat -c "%F" /dev/sdb || false
        lxc storage volume delete "${poolName}" vol1

        lxc config device add v1 block1 disk source="/tmp/lxd-test-${poolName}/lxd-test-block" readonly=true
        sleep 7
        [ "$(lxc exec v1 -- cat /sys/block/sdb/ro)" -eq 1 ]
        lxc config device set v1 block1 readonly=false
        sleep 7
        [ "$(lxc exec v1 -- cat /sys/block/sdb/ro)" -eq 0 ]

        # Hotplugging directories is not allowed and will fail
        ! lxc config device add v1 dir1 disk source="/tmp/lxd-test-${poolName}" || false

        # Hot plug cloud-init:config ISO.
        lxc config device add v1 cloudinit disk source=cloud-init:config
        sleep 3
        lxc exec v1 -- mount -t iso9660 -o ro /dev/sr0 /mnt
        lxc exec v1 -- umount /dev/sr0
        lxc config device remove v1 cloudinit
        ! lxc exec v1 -- stat /dev/sr0 || false

        # Remove temporary directory.
        echo "==> Stopping VM"
        lxc stop -f v1
        rm "/tmp/lxd-test-${poolName}/lxd-test-block"
        rmdir "/tmp/lxd-test-${poolName}"

        echo "==> Deleting VM"
        lxc delete -f v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Change volume.size on pool from default of 10GiB to 4GiB and create VM"
                lxc storage set "${poolName}" volume.size 4GiB
        else
                echo "==> Change volume.size on pool from default of 8GiB to 16GiB and create VM"
                lxc storage set "${poolName}" volume.size 16GiB
        fi
        lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}"
        lxc start v1
        waitInstanceReady v1
        lxc info v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking VM root disk size is 4GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "4" ]
        else
                echo "==> Checking VM root disk size is 16GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "16" ]
        fi

        echo "==> Deleting VM and reset pool volume.size"
        lxc delete -f v1
        lxc storage unset "${poolName}" volume.size

        if [ "${poolDriver}" = "lvm" ]; then
                echo "==> Change volume.block.filesystem on pool and create VM"
                lxc storage set "${poolName}" volume.block.filesystem xfs
                lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}"
                lxc start v1
                waitInstanceReady v1
                lxc info v1

                echo "==> Checking VM config disk filesyste is XFS"
                serverPID="$(lxc query /1.0 | jq .environment.server_pid)"
                nsenter -m -t "${serverPID}" stat -f -c %T /var/snap/lxd/common/lxd/virtual-machines/v1 | grep -xF xfs

                echo "==> Deleting VM"
                lxc delete -f v1
                lxc storage unset "${poolName}" volume.block.filesystem
        fi

        lxc profile copy default vmsmall
        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Create VM from profile with small disk size (3GiB)"
                lxc profile device add vmsmall root disk pool="${poolName}" path=/ size=3GiB
        else
                echo "==> Create VM from profile with small disk size (8GiB)"
                lxc profile device add vmsmall root disk pool="${poolName}" path=/ size=8GiB
        fi
        lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -p vmsmall
        lxc start v1
        waitInstanceReady v1
        lxc info v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking VM root disk size is 3GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "3" ]
        else
                echo "==> Checking VM root disk size is 8GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "8" ]
        fi
        lxc stop -f v1

        echo "==> Copy to different storage pool with same driver and check size"
        if [ "${poolDriver}" = "dir" ] || [ "${poolDriver}" = "ceph" ]; then
                lxc storage create "${poolName}2" "${poolDriver}"
        elif [ "${poolDriver}" = "lvm" ]; then
                lxc storage create "${poolName}2" "${poolDriver}" size=40GiB lvm.use_thinpool=false
        elif [ "${poolDriver}" = "lvm-thin" ]; then
                lxc storage create "${poolName}2" lvm size=20GiB
        elif [ "${poolDriver}" = "powerflex" ]; then
                createPowerFlexPool "${poolName}2"
        else
                lxc storage create "${poolName}2" "${poolDriver}" size=20GiB
        fi

        lxc copy v1 v2 -s "${poolName}2"
        lxc start v2
        waitInstanceReady v2
        lxc info v2

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking copied VM root disk size is 3GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "3" ]
        else
                echo "==> Checking copied VM root disk size is 8GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "8" ]
        fi
        lxc delete -f v2
        lxc storage delete "${poolName}2"

        echo "==> Copy to different storage pool with different driver and check size"
        dstPoolDriver=zfs # Use ZFS storage pool as that has fixed volumes not files.
        if [ "${poolDriver}" = "zfs" ]; then
                dstPoolDriver=lvm # Use something different when testing ZFS.
        fi

        lxc storage create "${poolName}2" "${dstPoolDriver}" size=20GiB
        lxc copy v1 v2 -s "${poolName}2"
        lxc start v2
        waitInstanceReady v2
        lxc info v2

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking copied VM root disk size is 3GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "3" ]
        else
                echo "==> Checking copied VM root disk size is 8GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "8" ]
        fi
        lxc delete -f v2

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Override volume size from profile (4GiB) to 5GiB and copy to different storage pool"
                lxc config device override v1 root size=5GiB
        else
                echo "==> Override volume size from profile (8GiB) to 16GiB and copy to different storage pool"
                lxc config device override v1 root size=16GiB
        fi
        lxc copy v1 v2 -s "${poolName}2"
        lxc start v2
        waitInstanceReady v2
        lxc info v2

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking copied VM root disk size is 5GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "5" ]
        else
                echo "==> Checking copied VM root disk size is 16GiB"
                [ "$(($(lxc exec v2 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "16" ]
        fi
        lxc delete -f v2
        lxc storage delete "${poolName}2"

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Publishing larger VM (5GiB)"
        else
                echo "==> Publishing larger VM (16GiB)"
        fi
        lxc publish v1 --alias vmbig
        lxc delete -f v1
        if [ "${poolDriver}" != "powerflex" ]; then
                lxc storage set "${poolName}" volume.size 3GiB
        else
                lxc storage set "${poolName}" volume.size 8GiB
        fi

        echo "==> Check VM create fails when image larger than volume.size"
        ! lxc init vmbig v1 --vm -s "${poolName}" || false

        echo "==> Check VM create succeeds when no volume.size set"
        lxc storage unset "${poolName}" volume.size
        lxc init vmbig v1 --vm -s "${poolName}"
        lxc start v1
        waitInstanceReady v1
        lxc info v1

        if [ "${poolDriver}" != "powerflex" ]; then
                echo "==> Checking new VM root disk size has default volume size of 10GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "10" ]
        else
                echo "==> Checking new VM root disk size has default volume size of 16GiB"
                [ "$(($(lxc exec v1 -- blockdev --getsize64 /dev/sda) / GiB))" -eq "16" ]
        fi

        echo "===> Renaming VM"
        lxc stop -f v1
        lxc rename v1 v1renamed

        echo "==> Deleting VM, vmbig image and vmsmall profile"
        lxc delete -f v1renamed
        lxc image delete vmbig
        lxc profile delete vmsmall

        echo "==> Checking VM Generation UUID with QEMU"
        lxc init "${TEST_IMG:-ubuntu-minimal-daily:22.04}" v1 --vm -s "${poolName}"
        lxc start v1
        waitInstanceReady v1
        lxc info v1

        # Check that the volatile.uuid.generation setting is applied to the QEMU process.
        vmGenID=$(lxc config get v1 volatile.uuid.generation)
        qemuGenID=$(awk '/driver = "vmgenid"/,/guid = / {print $3}' /var/snap/lxd/common/lxd/logs/v1/qemu.conf | sed -n 's/"\([0-9a-fA-F]\{8\}-[0-9a-fA-F]\{4\}-[0-9a-fA-F]\{4\}-[0-9a-fA-F]\{4\}-[0-9a-fA-F]\{12\}\)"/\1/p')
        if [ "${vmGenID}" != "${qemuGenID}" ]; then
                echo "==> VM Generation ID in LXD config does not match VM Generation ID in QEMU process"
                false
        fi

        lxc delete -f v1

        echo "==> Deleting storage pool"
        lxc storage delete "${poolName}"
        echo "::endgroup::"
done

echo "==> Delete network"
lxc profile device remove default eth0
lxc network delete lxdbr0

# shellcheck disable=SC2034
FAIL=0
